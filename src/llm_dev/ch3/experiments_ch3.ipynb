{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56af7df9",
   "metadata": {},
   "source": [
    "### Simple self-attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a922a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "text = \"Your journey starts with one step\"\n",
    "# assume the embedding layer is initialized with the following values\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],\n",
    "        [0.55, 0.87, 0.66],\n",
    "        [0.57, 0.85, 0.64],\n",
    "        [0.22, 0.58, 0.33],\n",
    "        [0.77, 0.25, 0.10],\n",
    "        [0.05, 0.80, 0.55]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f3df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c292be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f56e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(inputs[1], x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334eace7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b122be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ccbc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8a6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# attn_weights_2_2 = torch.cat([attn_weights_2] * 1000)\n",
    "# context_vector_2 = attn_weights_2_2 @ torch.cat([inputs] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cbd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector_2 = attn_weights_2 @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a138449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c95145",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = inputs @ inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9646c702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de7ca258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "assert np.allclose(attn_scores[1], attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cac7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5fb28bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8100c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(attn_weights_2, attn_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ca82986",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(torch.sum(attn_weights, dim=-1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6ccdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors = attn_weights @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c51c4b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "232efcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(context_vectors[1], context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a68b6",
   "metadata": {},
   "source": [
    "### Trainable Weights Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "976ecd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "torch.manual_seed(123)\n",
    "W_k = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_v = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_q = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70bf7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff94e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_2 = x_2 @ W_k\n",
    "value_2 = x_2 @ W_k\n",
    "query_2 = x_2 @ W_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d006b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4107, 0.6274])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8ec45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "queries = inputs @ W_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5d56430",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = queries @ keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c90fe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2118, -0.1385, -0.1361, -0.0602, -0.0547, -0.0894],\n",
       "        [-0.3533, -0.4847, -0.4709, -0.2879, -0.0888, -0.4388],\n",
       "        [-0.3491, -0.4829, -0.4691, -0.2874, -0.0877, -0.4381],\n",
       "        [-0.2002, -0.2877, -0.2794, -0.1728, -0.0502, -0.2635],\n",
       "        [-0.1753, -0.3144, -0.3046, -0.1974, -0.0434, -0.3020],\n",
       "        [-0.2533, -0.3215, -0.3126, -0.1871, -0.0639, -0.2848]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "890744eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores / keys.shape[1]**0.5, dim=-1)\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cba3781d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1555, 0.1638, 0.1641, 0.1731, 0.1738, 0.1696],\n",
       "        [0.1660, 0.1512, 0.1527, 0.1738, 0.2001, 0.1562],\n",
       "        [0.1663, 0.1512, 0.1527, 0.1737, 0.2000, 0.1561],\n",
       "        [0.1674, 0.1574, 0.1583, 0.1707, 0.1861, 0.1601],\n",
       "        [0.1719, 0.1559, 0.1569, 0.1693, 0.1888, 0.1572],\n",
       "        [0.1644, 0.1567, 0.1577, 0.1723, 0.1880, 0.1608]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9857b6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b5da70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = attention_weights @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a290dedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1405, -0.5932])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12293c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
