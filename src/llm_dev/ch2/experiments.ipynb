{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_dev.ch2.simple_tokenizer import SimpleTokenizerV1\n",
    "from llm_dev.ch2.read_text import read_text_file\n",
    "from llm_dev.ch2 import pre_process_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../../../the-verdict.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4690\n",
      "The generated vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "full_text = read_text_file(input_file)\n",
    "vocab = pre_process_text.create_vocab(full_text)\n",
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 7\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/arangodb/repos/llm-dev/src/llm_dev/ch2/simple_tokenizer.py:13\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[32m     12\u001b[39m     preprocessed = split_text_to_tokens(text)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4690\n",
      "The generated vocabulary size: 1132\n"
     ]
    }
   ],
   "source": [
    "vocab_v2 = pre_process_text.create_vocab_v2(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('younger', 1127)\n",
      "1 ('your', 1128)\n",
      "2 ('yourself', 1129)\n",
      "3 ('<|unk|>', 1130)\n",
      "4 ('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab_v2.items())[-5:]):\n",
    "    print(i, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version('tiktoken'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 11531, 558, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPalace.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPalace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "print(tokenizer.decode(integers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 2.1 Byte Pair encoding of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(\"Akwirw ier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ak\n",
      "w\n",
      "ir\n",
      "w\n",
      " \n",
      "ier\n"
     ]
    }
   ],
   "source": [
    "for single_id in ids:\n",
    "    print(tokenizer.decode([single_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akwirw ier'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.6: Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text_file(input_file)\n",
    "enco_text = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enco_text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [40, 367, 2885, 1464]\n",
      "y:     [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "print(f\"x: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Use data loader with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_dev.ch2.data_loader import create_dataloader_v1\n",
    "\n",
    "\n",
    "data_loader = create_dataloader_v1(\n",
    "    txt=raw_text,\n",
    "    batch_size=3,\n",
    "    max_length=4,\n",
    "    stride=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "data_iter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632]])]\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645]]), tensor([[ 438, 2016,  257,  922],\n",
      "        [5891, 1576,  438,  568],\n",
      "        [ 340,  373,  645, 1049]])]\n"
     ]
    }
   ],
   "source": [
    "batch_2 = next(data_iter)\n",
    "print(batch_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.7: Text Embedding\n",
    "A hands-on example on how embedding layer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9269,  1.4873, -0.4974],\n",
      "        [ 0.4396, -0.7581,  1.0783],\n",
      "        [ 0.8008,  1.6806,  0.3559],\n",
      "        [-0.6866,  0.6105,  1.3347],\n",
      "        [-0.2316,  0.0418, -0.2516],\n",
      "        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(42)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6866,  0.6105,  1.3347]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8008,  1.6806,  0.3559],\n",
      "        [-0.6866,  0.6105,  1.3347],\n",
      "        [ 0.8599, -0.3097, -0.3957],\n",
      "        [ 0.4396, -0.7581,  1.0783]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8: Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    txt=raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs: \\n\", inputs)\n",
    "print(\"Inputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(targets)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "from turtle import pos\n",
    "\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2192, -0.2741,  0.6823, -0.1669, -0.8921,  0.1055, -0.3790,  0.5566,\n",
       "        -0.6532,  0.6945,  0.2294,  0.7706, -1.0600,  1.1109, -0.0416,  0.0755,\n",
       "        -0.4250, -0.6974,  1.9947, -0.1386,  0.6699, -0.1933, -0.6626, -2.5877,\n",
       "         0.1789,  1.3156,  1.2939, -0.6690, -0.7701, -0.2048,  2.0718, -1.4005,\n",
       "        -0.1533, -0.9274, -1.6961,  0.7975,  0.1333, -0.3854,  1.7930, -0.5936,\n",
       "         1.0393, -0.2826, -0.7999, -0.3349,  0.3119,  1.9270,  0.6508,  1.3521,\n",
       "        -0.5486, -0.0687, -0.5394,  1.2094,  1.2658, -0.6468,  0.9397, -0.2972,\n",
       "         1.5191, -0.9091,  0.3122, -0.8711,  1.0148, -0.5497,  2.1036, -0.6052,\n",
       "         0.6682,  0.8231,  1.1816,  1.1979,  0.8280, -1.4040,  1.0860,  1.1826,\n",
       "         0.5376, -0.5385, -0.0396,  0.2138, -0.1422,  0.9105, -0.0212, -1.2918,\n",
       "         0.4179,  0.8971, -0.9209, -0.1980,  0.3185,  0.7440, -0.8477, -0.8741,\n",
       "         1.1092, -0.0516,  0.5348, -0.4087,  1.0480,  1.3445,  0.8101,  0.9920,\n",
       "         1.0377, -0.2992, -0.8859,  0.1964, -1.0400, -0.7561,  0.3840, -1.4197,\n",
       "        -0.1261,  1.7196,  0.9147,  1.3658,  0.3674, -0.7225,  0.9479, -0.4274,\n",
       "        -0.2308, -0.4608,  0.7543,  0.6834, -1.9253,  0.9365,  0.8931, -0.2159,\n",
       "        -0.6158,  0.9734, -2.5512,  0.0662,  0.9031,  0.8408, -0.4241, -0.6860,\n",
       "         0.3398,  0.2276, -1.7890, -0.0408,  0.4250, -0.2091, -0.9483,  0.0446,\n",
       "        -1.3037, -1.7488, -0.8444, -0.6601,  0.2241,  0.1066,  0.2360, -1.2412,\n",
       "        -1.2310,  0.9888,  0.1138, -0.7509,  1.8058,  0.5504,  0.7749, -0.6269,\n",
       "        -0.9417, -0.4158,  0.8920,  0.8680,  0.0459, -0.7903, -0.0950, -0.5911,\n",
       "         1.9361,  0.1717, -0.1643, -0.5683, -1.6081,  0.3665,  1.1954, -0.7840,\n",
       "         0.7066, -1.5374, -0.2583, -0.2902, -0.0753,  0.0936, -0.0415,  0.6072,\n",
       "        -0.2288,  1.6960, -0.8879,  0.5850, -0.0230, -1.0637, -1.0063,  0.9072,\n",
       "        -0.9650,  0.0294,  0.1153, -0.8793,  1.2674,  0.0163,  0.5667, -0.1067,\n",
       "        -0.7883, -1.1618, -0.1707,  0.1062, -0.0304,  1.1064,  0.0476,  2.1847,\n",
       "         0.1250,  0.7394, -0.2672,  0.2419, -1.5063, -0.1487, -0.3152, -0.8719,\n",
       "        -0.5228,  1.5819, -0.4983, -0.1640, -0.3644,  0.6577, -1.1073, -0.0338,\n",
       "        -2.8117,  1.4309,  0.5757, -0.2500, -0.3666, -0.7455, -0.1223,  1.6024,\n",
       "         0.7246, -0.0747, -0.1495,  0.0274, -0.6572,  1.6697,  0.0580,  1.0368,\n",
       "        -1.2652, -0.1410, -0.3261,  0.3074, -0.9010, -0.6194, -0.5071,  1.2971,\n",
       "        -1.4047, -0.0815, -0.2763, -0.8060,  0.7520,  0.9157,  1.6787, -1.3949,\n",
       "         0.8841, -0.2070,  0.7083,  0.2372, -0.5760, -2.0313, -0.3160, -0.2499],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
